{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d75b0238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in ./.venv/lib/python3.12/site-packages (1.108.1)\n",
      "Collecting python-dotenv\n",
      "  Using cached python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting langchain_openai\n",
      "  Downloading langchain_openai-0.3.33-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./.venv/lib/python3.12/site-packages (from openai) (4.10.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.12/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./.venv/lib/python3.12/site-packages (from openai) (0.11.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./.venv/lib/python3.12/site-packages (from openai) (2.11.9)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.12/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in ./.venv/lib/python3.12/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in ./.venv/lib/python3.12/site-packages (from openai) (4.15.0)\n",
      "Collecting langchain-core<1.0.0,>=0.3.76 (from langchain_openai)\n",
      "  Downloading langchain_core-0.3.76-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting tiktoken<1,>=0.7 (from langchain_openai)\n",
      "  Downloading tiktoken-0.11.0-cp312-cp312-macosx_10_13_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: idna>=2.8 in ./.venv/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Collecting langsmith>=0.3.45 (from langchain-core<1.0.0,>=0.3.76->langchain_openai)\n",
      "  Downloading langsmith-0.4.31-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0 (from langchain-core<1.0.0,>=0.3.76->langchain_openai)\n",
      "  Using cached tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<1.0.0,>=0.3.76->langchain_openai)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting PyYAML>=5.3 (from langchain-core<1.0.0,>=0.3.76->langchain_openai)\n",
      "  Downloading pyyaml-6.0.3-cp312-cp312-macosx_10_13_x86_64.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: packaging>=23.2 in ./.venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.76->langchain_openai) (25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./.venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./.venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "Collecting regex>=2022.1.18 (from tiktoken<1,>=0.7->langchain_openai)\n",
      "  Using cached regex-2025.9.18-cp312-cp312-macosx_10_13_x86_64.whl.metadata (40 kB)\n",
      "Collecting requests>=2.26.0 (from tiktoken<1,>=0.7->langchain_openai)\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.76->langchain_openai)\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting orjson>=3.9.14 (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain_openai)\n",
      "  Downloading orjson-3.11.3-cp312-cp312-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl.metadata (41 kB)\n",
      "Collecting requests-toolbelt>=1.0.0 (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain_openai)\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting zstandard>=0.23.0 (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.76->langchain_openai)\n",
      "  Downloading zstandard-0.25.0-cp312-cp312-macosx_10_13_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai)\n",
      "  Using cached charset_normalizer-3.4.3-cp312-cp312-macosx_10_13_universal2.whl.metadata (36 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Using cached python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Downloading langchain_openai-0.3.33-py3-none-any.whl (74 kB)\n",
      "Downloading langchain_core-0.3.76-py3-none-any.whl (447 kB)\n",
      "Downloading tiktoken-0.11.0-cp312-cp312-macosx_10_13_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading langsmith-0.4.31-py3-none-any.whl (386 kB)\n",
      "Downloading pyyaml-6.0.3-cp312-cp312-macosx_10_13_x86_64.whl (182 kB)\n",
      "Using cached regex-2025.9.18-cp312-cp312-macosx_10_13_x86_64.whl (289 kB)\n",
      "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Using cached tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Using cached charset_normalizer-3.4.3-cp312-cp312-macosx_10_13_universal2.whl (205 kB)\n",
      "Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading orjson-3.11.3-cp312-cp312-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl (238 kB)\n",
      "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Downloading zstandard-0.25.0-cp312-cp312-macosx_10_13_x86_64.whl (795 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m795.7/795.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: zstandard, urllib3, tenacity, regex, PyYAML, python-dotenv, orjson, jsonpointer, charset_normalizer, requests, jsonpatch, tiktoken, requests-toolbelt, langsmith, langchain-core, langchain_openai\n",
      "Successfully installed PyYAML-6.0.3 charset_normalizer-3.4.3 jsonpatch-1.33 jsonpointer-3.0.0 langchain-core-0.3.76 langchain_openai-0.3.33 langsmith-0.4.31 orjson-3.11.3 python-dotenv-1.1.1 regex-2025.9.18 requests-2.32.5 requests-toolbelt-1.0.0 tenacity-9.1.2 tiktoken-0.11.0 urllib3-2.5.0 zstandard-0.25.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install openai python-dotenv langchain_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0bb427",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe6ba5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under a moonlit sky, the gentle unicorn whispered dreams of stardust and rainbows to the sleepy forest, as the world around it slowly drifted into a peaceful slumber.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"API key not found. Please set the OPENAI_API_KEY environment variable.\")\n",
    "\n",
    "client = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "response = client.invoke(\n",
    "    input=\"Write a one-sentence bedtime story about a unicorn.\"\n",
    ")\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8013f134",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e9696c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_task = \"\"\"Suppose we have 8 square blocks, each colored either green (G), red (R), or white (W), and stacked on top of one another on the 2 x 2 grid formed by the four adjacent plane cells whose lower left coordinates are (0,0), (0,1), (1,0), and (1,1).\n",
    "The initial configuration is this:\n",
    "(0,0): [W, R] (meaning that cell (0,0) has a white block on it, and then a red block on top of that white block.\n",
    "(0,1): [G, G] (green on top of green)\n",
    "(1,0): [W, G] (green on top of white)\n",
    " (1,1): [W, W] (white on top of white)\n",
    "The only move you are allowed to make is to pick up one of the top blocks from a non-empty cell and put it op top of another top block. For example, applying the move (0,1) -> (1,1) to the initial configuration would produce the following configuration:\n",
    "(0,0): [W, R]\n",
    "(0,1): [G]\n",
    " (1,0): [W, G]\n",
    "(1,1): [W, W, G]\n",
    "Either give a sequence of moves resulting in a configuration where no white box is directly on top of another white box, or else prove that no such sequence exists.\n",
    "Think out your answer carefully and step-by-step and explain your reasoning.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ffaf3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To determine whether we can achieve a configuration where no white block is directly on top of another white block, let's analyze the initial configuration and consider possible moves.\n",
      "\n",
      "**Initial configuration:**\n",
      "- (0,0): [W, R]\n",
      "- (0,1): [G, G]\n",
      "- (1,0): [W, G]\n",
      "- (1,1): [W, W]\n",
      "\n",
      "Our goal: A configuration where no W block is directly on top of another W block.\n",
      "\n",
      "### Observations:\n",
      "1. In the initial configuration, (1,1) is the only cell with two white blocks consecutively.\n",
      "2. We must move at least one of these blocks to break the W over W configuration at (1,1).\n",
      "\n",
      "### Strategy:\n",
      "- We need to move blocks around without recreating the situation of having W directly on top of W.\n",
      "- We should also aim to evenly distribute blocks to facilitate more moves, if necessary.\n",
      "\n",
      "### Execution:\n",
      "1. **Move from (0,1) to (1,1):**\n",
      "   - Take the top G from (0,1) and place it on (1,1).\n",
      "   - New configuration:\n",
      "     - (0,0): [W, R]\n",
      "     - (0,1): [G]\n",
      "     - (1,0): [W, G]\n",
      "     - (1,1): [W, W, G]\n",
      "\n",
      "2. **Move from (1,1) to (0,1):**\n",
      "   - Move the top G from (1,1) to (0,1).\n",
      "   - New configuration:\n",
      "     - (0,0): [W, R]\n",
      "     - (0,1): [G, G]\n",
      "     - (1,0): [W, G]\n",
      "     - (1,1): [W, W]\n",
      "\n",
      "3. **Move from (1,1) to (0,0):**\n",
      "   - Move the top W from (1,1) to (0,0).\n",
      "   - New configuration:\n",
      "     - (0,0): [W, R, W]\n",
      "     - (0,1): [G, G]\n",
      "     - (1,0): [W, G]\n",
      "     - (1,1): [W]\n",
      "\n",
      "4. **Move from (0,0) to (1,1):**\n",
      "   - Move the top W from (0,0) to (1,1).\n",
      "   - New configuration:\n",
      "     - (0,0): [W, R]\n",
      "     - (0,1): [G, G]\n",
      "     - (1,0): [W, G]\n",
      "     - (1,1): [W, W]\n",
      "\n",
      "5. **Move from (1,1) to (1,0):**\n",
      "   - Move the top W from (1,1) to (1,0).\n",
      "   - New configuration:\n",
      "     - (0,0): [W, R]\n",
      "     - (0,1): [G, G]\n",
      "     - (1,0): [W, G, W]\n",
      "     - (1,1): [W]\n",
      "\n",
      "6. **Move from (0,1) to (1,1):**\n",
      "   - Move the top G from (0,1) to (1,1).\n",
      "   - New configuration:\n",
      "     - (0,0): [W, R]\n",
      "     - (0,1): [G]\n",
      "     - (1,0): [W, G, W]\n",
      "     - (1,1): [W, G]\n",
      "\n",
      "7. **Move from (1,0) to (0,1):**\n",
      "   - Move the top W from (1,0) to (0,1).\n",
      "   - New configuration:\n",
      "     - (0,0): [W, R]\n",
      "     - (0,1): [G, W]\n",
      "     - (1,0): [W, G]\n",
      "     - (1,1): [W, G]\n",
      "\n",
      "At this stage, no stack has W directly on top of another W. Thus, a sequence of moves that avoids W on W is possible.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = client.invoke(\n",
    "    input=gpt_task\n",
    ")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e06cee",
   "metadata": {},
   "source": [
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3dd62179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Wegańskie Curry z Ciecierzycą i Szpinakiem**\n",
      "\n",
      "**Składniki:**\n",
      "- 1 łyżka oleju kokosowego\n",
      "- 1 duża cebula, posiekana\n",
      "- 3 ząbki czosnku, drobno posiekane\n",
      "- 1 kawałek imbiru (około 2 cm), starty\n",
      "- 1 łyżka pasty curry\n",
      "- 1 puszka (400 g) pomidorów krojonych\n",
      "- 1 puszka (400 g) mleka kokosowego\n",
      "- 1 puszka (400 g) ciecierzycy, odsączonej i przepłukanej\n",
      "- 2 szklanki świeżego szpinaku\n",
      "- Sól i pieprz do smaku\n",
      "- Świeża kolendra do dekoracji (opcjonalnie)\n",
      "- Ryż lub chleb naan do podania\n",
      "\n",
      "**Przygotowanie:**\n",
      "1. Rozgrzej olej kokosowy w dużym rondlu na średnim ogniu.\n",
      "2. Dodaj cebulę i smaż przez około 5 minut, aż zmięknie.\n",
      "3. Dodaj czosnek i imbir, smaż przez 1 minutę.\n",
      "4. Dodaj pastę curry, wymieszaj i gotuj przez kolejną minutę.\n",
      "5. Wlej pomidory i mleko kokosowe, wymieszaj.\n",
      "6. Dodaj ciecierzycę. Doprowadź do wrzenia.\n",
      "7. Zmniejsz ogień i gotuj na wolnym ogniu przez 15 minut.\n",
      "8. Dodaj szpinak, wymieszaj i gotuj, aż zmięknie (około 2 minuty).\n",
      "9. Dopraw solą i pieprzem do smaku.\n",
      "10. Podawaj z ryżem lub chlebem naan, posypane kolendrą.\n",
      "\n",
      "**Czas przygotowania i gotowania:** około 30 minut\n",
      "\n",
      "Smacznego!\n"
     ]
    }
   ],
   "source": [
    "cooking_task = \"\"\"Jesteś doświadczonym kucharzem specjalizującym się w kuchni wegańskiej. Przygotuj szczegółowy przepis na wegańskie danie. Ogranicz przepis do 200 słów. Uwzględnij listę składników, kroki przygotowania oraz czas gotowania. Upewnij się, że przepis jest łatwy do zrozumienia i wykonania przez osoby o różnym poziomie umiejętności kulinarnych.\"\"\"\n",
    "response = client.invoke(\n",
    "    input=cooking_task\n",
    ")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d231e9",
   "metadata": {},
   "source": [
    "## Exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c66eb942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Łyk szczęścia, siadaj i chilluj!\"\n"
     ]
    }
   ],
   "source": [
    "coffe_task = \"\"\"Jesteś expertem w tworzeniu sloganów reklamowych. Na podstawie przykładów stwórz nowy slogan reklamowy dla kawiarni.\n",
    "Przykłady sloganów:\n",
    "1. Dupnij se kawke byczku.\n",
    "2. GenZ coffee in the mini.\n",
    "Teksty powinny być podobne do podanych przykładów.\n",
    "Odpowiedź:\"\"\"\n",
    "response = client.invoke(\n",
    "    input=coffe_task\n",
    ")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0cda422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oczywiście! Oto trzy pomysły na edukacyjne gry dla dzieci:\n",
      "\n",
      "1. {\n",
      "   \"name\": \"Matematyczne Podchody\",\n",
      "   \"description\": \"Gra łącząca zadania matematyczne z zabawą w terenie.\",\n",
      "   \"min_number_of_players\": 2,\n",
      "   \"max_number_of_players\": 4,\n",
      "   \"rules\": \"Każdy gracz otrzymuje zestaw zadań matematycznych na kartkach. Zadania są umieszczone w różnych miejscach na podwórku lub w domu. Gracz musi najpierw rozwiązać zadanie, aby otrzymać wskazówkę dotyczącą miejsca następnej kartki. Gra kończy się, gdy któryś z graczy rozwiąże wszystkie zadania i wróci na miejsce startu.\"\n",
      "}\n",
      "\n",
      "2. {\n",
      "   \"name\": \"Językowe Memory\",\n",
      "   \"description\": \"Uczy nowych słówek poprzez klasyczną grę w memory.\",\n",
      "   \"min_number_of_players\": 2,\n",
      "   \"max_number_of_players\": 4,\n",
      "   \"rules\": \"Na kartonikach znajdują się pary obrazków i słów w obcym języku. Gracze na zmianę odkrywają po dwa kartoniki. Jeśli trafiają na parę obrazek-słowo, zabierają kartoniki. Wygrywa ten, kto zbierze najwięcej par. Można dostosować poziom trudności poprzez użycie prostszych lub trudniejszych słów.\"\n",
      "}\n",
      "\n",
      "3. {\n",
      "   \"name\": \"Ekologiczna Misja\",\n",
      "   \"description\": \"Gra planszowa ucząca dzieci zasad ekologii i ochrony środowiska.\",\n",
      "   \"min_number_of_players\": 2,\n",
      "   \"max_number_of_players\": 4,\n",
      "   \"rules\": \"Gracze przesuwają swoje pionki po planszy składającej się z różnych pól: wyzwań ekologicznych, pytań i akcji. Na polach wyzwań gracze muszą wykonać zadania związane z recyklingiem lub oszczędzaniem energii. Na polach pytań odpowiadają na pytania dotyczące środowiska. Jeśli odpowiedzą prawidłowo, posuwają się dalej. Wygrywa gracz, który jako pierwszy dotrze do końca planszy, wykonując wszystkie zadania.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "game_task = \"\"\"Jesteś ekspertem od gier edukacyjnych dla dzieci. Podaj trzy pomysły na gry według poniższego formatu:\n",
    "{\n",
    "\"name\": \"game name\",\n",
    "\"description\": \"game description\",\n",
    "\"min_number_of_players\": 2,\n",
    "\"max_number_of_players\": 4,\n",
    "\"rules\": \"game rules\"\n",
    "}\"\"\"\n",
    "response = client.invoke(\n",
    "    input=game_task\n",
    ")\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
